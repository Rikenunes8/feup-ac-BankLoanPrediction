{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "data = pd.read_csv('data_processed/data.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_ids = [c for c in data.columns if c[-3:] != '_id' and c != 'code']\n",
    "data = data[no_ids]\n",
    "\n",
    "categorical_columns = list(data.select_dtypes(\"object\").columns)\n",
    "print(categorical_columns)\n",
    "\n",
    "def get_features(df):\n",
    "    return df.drop('status', axis=1).values\n",
    "def get_target(df):\n",
    "    return df['status'].values\n",
    "\n",
    "display(data.info())\n",
    "\n",
    "\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(features, target, test_size=0.2, random_state=1):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=test_size, random_state=random_state)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(X_train, X_test, scaler):\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "    return X_train, X_test\n",
    "\n",
    "def standardize_data(X_train, X_test):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    normalize_data(X_train, X_test, StandardScaler())\n",
    "def min_max_scaling(X_train, X_test):\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    normalize_data(X_train, X_test, MinMaxScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(df, columns):\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    for col in columns:\n",
    "        if (col in df.keys()):\n",
    "            df[col] = le.fit_transform(df[col])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def add_model(name, model):\n",
    "    df = data.copy()\n",
    "    if (name == 'dtc'):\n",
    "        df.drop(['age_on_loan_request_disc'], inplace=True, axis=1)\n",
    "    else:\n",
    "        df.drop(['age_on_loan_request'], inplace=True, axis=1)\n",
    "\n",
    "    df = encode_data(df, categorical_columns)\n",
    "    X_train, X_test, y_train, y_test = split_data(get_features(df), get_target(df))\n",
    "    # X_train, X_test = standardize_data(X_train, X_test)\n",
    "    # X_train, X_test = min_max_scaling(X_train, X_test)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    results[name] = {'model': model, \n",
    "                    'X_train': X_train, \n",
    "                    'X_test': X_test, \n",
    "                    'y_train': y_train, \n",
    "                    'y_test': y_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "add_model('dtc',\n",
    "    DecisionTreeClassifier()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "add_model('rf',\n",
    "    RandomForestClassifier()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "add_model('svc',\n",
    "    SVC(probability=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(name):\n",
    "    result = results[name]\n",
    "    pred = result['model'].predict(result['X_test'])\n",
    "    result['pred'] = pred\n",
    "\n",
    "for name in results.keys():\n",
    "    predict(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(name):\n",
    "    result = results[name]\n",
    "    proba = result['model'].predict_proba(result['X_test'])\n",
    "    result['pred_prob'] = proba\n",
    "\n",
    "for name in results.keys():\n",
    "    predict_proba(name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(cm, i):\n",
    "    return cm[i][i]/sum(cm[i])\n",
    "\n",
    "def precision(cm, i):\n",
    "    cmt = np.copy(cm).transpose()\n",
    "    return cmt[i][i]/sum(cmt[i])\n",
    "\n",
    "def f_measure(cm, i):\n",
    "    p = precision(cm, i)*100\n",
    "    r = recall(cm, i)*100\n",
    "    return 2 * (p * r) / (p + r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "def conf_matrix(y_test, y_pred):\n",
    "    cm =  confusion_matrix(y_test, y_pred)\n",
    "    '''\n",
    "    print(\"TP:\", cm[1][1])\n",
    "    print(\"TN:\", cm[0][0])\n",
    "    print(\"FP:\", cm[0][1])\n",
    "    print(\"FN:\", cm[1][0])\n",
    "    '''\n",
    "    ConfusionMatrixDisplay(cm, display_labels=['True', 'False']).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(name):\n",
    "    result = results[name]\n",
    "    result['score'] = result['model'].score(result['X_test'], result['y_test'])\n",
    "    print(name.upper()+\":\", result['score'])\n",
    "\n",
    "for name in results.keys():\n",
    "    score(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = 0.50\n",
    "\n",
    "for name in results.keys():\n",
    "    result = results[name]\n",
    "    conf_matrix(result['y_test'], np.where(result['pred_prob'][:,-1] > prob, 1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, RocCurveDisplay, auc\n",
    "\n",
    "def roc_and_auc(name):\n",
    "    result = results[name]\n",
    "    fpr, tpr, thresholds = roc_curve(result['y_test'], result['pred_prob'][:,0], pos_label=result['model'].classes_[0])\n",
    "    roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()\n",
    "    return print(name.upper() + \":\", auc(fpr, tpr))\n",
    "\n",
    "for name in results.keys():\n",
    "    roc_and_auc(name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
