{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "data = pd.read_csv('data_processed/data.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_ids = [c for c in data.columns if c[-3:] != '_id' and c != 'code']\n",
    "data = data[no_ids]\n",
    "data.drop(['account_frequency', 'gender', 'card_type'], axis=1, inplace=True)\n",
    "display(data.head(10))\n",
    "\n",
    "categorical_columns = list(data.select_dtypes(\"object\").columns)\n",
    "\n",
    "def get_features(df):\n",
    "    return df.drop('status', axis=1).values\n",
    "def get_target(df):\n",
    "    return df['status'].values\n",
    "\n",
    "display(data.info())\n",
    "\n",
    "\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(features, target, test_size=0.2, random_state=1):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=test_size, random_state=random_state)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(X_train, X_test, scaler):\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "    return X_train, X_test\n",
    "\n",
    "def standardize_data(X_train, X_test):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    normalize_data(X_train, X_test, StandardScaler())\n",
    "def min_max_scaling(X_train, X_test):\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    normalize_data(X_train, X_test, MinMaxScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(df, columns):\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    for col in columns:\n",
    "        if (col in df.keys()):\n",
    "            df[col] = le.fit_transform(df[col])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_model(name, model):\n",
    "    df = data.copy()\n",
    "    if (name == 'dtc'):\n",
    "        df.drop(['age_on_loan_request_disc'], inplace=True, axis=1)\n",
    "    else:\n",
    "        df.drop(['age_on_loan_request'], inplace=True, axis=1)\n",
    "\n",
    "    df = encode_data(df, categorical_columns)\n",
    "    X_train, X_test, y_train, y_test = split_data(get_features(df), get_target(df))\n",
    "    # X_train, X_test = standardize_data(X_train, X_test)\n",
    "    # X_train, X_test = min_max_scaling(X_train, X_test)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    results[name] = {'model': model, \n",
    "                    'X_train': X_train, \n",
    "                    'X_test': X_test, \n",
    "                    'y_train': y_train, \n",
    "                    'y_test': y_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "add_model('dtc',\n",
    "    DecisionTreeClassifier()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "add_model('knn',\n",
    "    KNeighborsClassifier()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "add_model('mlp',\n",
    "    MLPClassifier()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "add_model('gnb',\n",
    "    GaussianNB()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "add_model('svc',\n",
    "    SVC(probability=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "add_model('rf',\n",
    "    RandomForestClassifier()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(name, isTrain=False):\n",
    "    prefix = 'train' if isTrain else 'test'\n",
    "    result = results[name]\n",
    "    pred = result['model'].predict(result['X_'+prefix])\n",
    "    result[prefix+'pred'] = pred\n",
    "\n",
    "for name in results.keys():\n",
    "    predict(name)\n",
    "for name in results.keys():\n",
    "    predict(name, isTrain=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(name, isTrain=False):\n",
    "    prefix = 'train' if isTrain else 'test'\n",
    "    result = results[name]\n",
    "    proba = result['model'].predict_proba(result['X_'+prefix])\n",
    "    result[prefix+'pred_prob'] = proba\n",
    "\n",
    "for name in results.keys():\n",
    "    predict_proba(name)\n",
    "for name in results.keys():\n",
    "    predict_proba(name, isTrain=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(cm, i):\n",
    "    return cm[i][i]/sum(cm[i]) if sum(cm[i]) > 0 else np.inf\n",
    "\n",
    "def precision(cm, i):\n",
    "    cmt = np.copy(cm).transpose()\n",
    "    return cm[i][i]/sum(cmt[i]) if sum(cmt[i]) > 0 else np.inf\n",
    "\n",
    "def f_measure(cm, i):\n",
    "    p = precision(cm, i)*100\n",
    "    r = recall(cm, i)*100\n",
    "    return 2 * (p * r) / (p + r) if p != np.inf and r != np.inf and p + r > 0 else np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.figure import Figure\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "def conf_matrix(y_test, y_pred, name, prefix):\n",
    "    cm =  confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=['True', 'False'])\n",
    "    disp.plot()\n",
    "    disp.ax_.set_title(name + ' ' + prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(name, isTrain=False):\n",
    "    prefix = 'train' if isTrain else 'test'\n",
    "    result = results[name]\n",
    "    result[prefix+'score'] = result['model'].score(result['X_'+prefix], result['y_'+prefix])\n",
    "    print(prefix, name.upper()+\":\\t\", result[prefix+'score'])\n",
    "\n",
    "print('\\tAccuracy')\n",
    "for name in results.keys():\n",
    "    score(name)\n",
    "for name in results.keys():\n",
    "    score(name, isTrain=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(name, isTrain=False):\n",
    "    prefix = 'train' if isTrain else 'test'\n",
    "    result = results[name]\n",
    "    cm =  confusion_matrix(result['y_'+prefix], result[prefix+'pred'])\n",
    "    print(prefix, name.upper()+\":\", '\\tRecall:',round(recall(cm, 0), 2), '\\t Precision:',round(precision(cm, 0), 2), '\\tF_Measure:',round(f_measure(cm, 0), 2))\n",
    "\n",
    "for name in results.keys():\n",
    "    evaluate(name)\n",
    "for name in results.keys():\n",
    "    evaluate(name, isTrain=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = 0.50\n",
    "\n",
    "prefixes = ('test', 'train')\n",
    "for prefix in prefixes[:1]:\n",
    "    for name in results.keys():\n",
    "        result = results[name]\n",
    "        prefix = 'test'\n",
    "        conf_matrix(result['y_'+prefix], np.where(result[prefix+'pred_prob'][:,-1] > prob, 1, -1), name, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, RocCurveDisplay, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def roc_and_auc(name, isTrain=False):\n",
    "    fig, ax = plt.subplots()\n",
    "    for prefix in ('test', 'train'):\n",
    "        result = results[name]\n",
    "        fpr, tpr, thresholds = roc_curve(result['y_'+prefix], result[prefix+'pred_prob'][:,0], pos_label=result['model'].classes_[0])\n",
    "        roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot(ax=ax,name=(name.upper()+' '+prefix))\n",
    "        print(name.upper(), prefix + \":\\t\", auc(fpr, tpr))\n",
    "\n",
    "for name in results.keys():\n",
    "    roc_and_auc(name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
