{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "data = pd.read_csv('data_processed/complete/data_selected.csv')\n",
    "\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(df):\n",
    "    return df.drop('status', axis=1)\n",
    "def get_target(df):\n",
    "    return df.drop(df.columns.difference(['status']), axis=1)\n",
    "\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(features, target, test_size=0.25, random_state=1):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=test_size, random_state=random_state)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Data shape:',data.shape)\n",
    "print('Status  1:',data[data['status']==1].shape)\n",
    "print('Status -1:',data[data['status']==-1].shape)\n",
    "\n",
    "_X_train, _X_test, _y_train, _y_test = split_data(get_features(data), get_target(data))\n",
    "\n",
    "print('\\nTrain shape:',_y_train.shape)\n",
    "print('Status ratio:',_y_train[_y_train['status']==1].shape[0],'|',_y_train[_y_train['status']==-1].shape[0])\n",
    "\n",
    "print('\\nTest shape:',_y_test.shape)\n",
    "print('Status ration:',_y_test[_y_test['status']==1].shape[0],'|',_y_test[_y_test['status']==-1].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversample(X_train, y_train):\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    sm = SMOTE(random_state=1, sampling_strategy=1.0)\n",
    "    X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_smote_X_train, _smote_y_train = oversample(_X_train, _y_train)\n",
    "\n",
    "print('\\nTrain shape:',_smote_y_train.shape)\n",
    "print('Status ratio:',_smote_y_train[_smote_y_train['status']==1].shape[0],'|',_smote_y_train[_smote_y_train['status']==-1].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(X_train, X_test, scaler):\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "    return X_train, X_test\n",
    "\n",
    "def standardize_data(X_train, X_test):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    return normalize_data(X_train, X_test, StandardScaler())\n",
    "def min_max_scaling(X_train, X_test):\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    return normalize_data(X_train, X_test, MinMaxScaler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_model(name, model, use_smote=False, scale=False):\n",
    "    import datetime\n",
    "    X_train = _X_train.copy() if not use_smote else _smote_X_train.copy()\n",
    "    y_train = _y_train.copy() if not use_smote else _smote_y_train.copy()\n",
    "    X_test = _X_test.copy()\n",
    "\n",
    "    # Scaling Normalizations\n",
    "    if scale == 'standard':\n",
    "        X_train, X_test = standardize_data(X_train, X_test)\n",
    "    elif scale == 'min_max':\n",
    "        X_train, X_test = min_max_scaling(X_train, X_test)\n",
    "    \n",
    "\n",
    "    start = datetime.datetime.now()\n",
    "    model.fit(X_train, np.ravel(y_train.values))\n",
    "    end = datetime.datetime.now()\n",
    "    \n",
    "    results[name] = {'model': model, \n",
    "                    'X_train': X_train, \n",
    "                    'X_test': X_test, \n",
    "                    'y_train': np.ravel(y_train.values), \n",
    "                    'y_test': np.ravel(_y_test.values),\n",
    "                    'fit_time': (end - start).microseconds / 1000 # time difference in milliseconds\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMOTE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "add_model('dtc',\n",
    "    DecisionTreeClassifier(),\n",
    "    use_smote=SMOTE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "add_model('knn',\n",
    "    KNeighborsClassifier(),\n",
    "    use_smote=SMOTE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "add_model('mlp',\n",
    "    MLPClassifier(),\n",
    "    use_smote=SMOTE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "add_model('gnb',\n",
    "    GaussianNB(),\n",
    "    use_smote=SMOTE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "add_model('svc',\n",
    "    SVC(probability=True),\n",
    "    use_smote=SMOTE,\n",
    "    scale='standard'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "add_model('lr',\n",
    "    LogisticRegression(),\n",
    "    use_smote=SMOTE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "add_model('rf',\n",
    "    RandomForestClassifier(),\n",
    "    use_smote=SMOTE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "add_model('gb',\n",
    "    GradientBoostingClassifier(),\n",
    "    use_smote=SMOTE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(name, isTrain=False):\n",
    "    prefix = 'train' if isTrain else 'test'\n",
    "    result = results[name]\n",
    "    pred = result['model'].predict(result['X_'+prefix])\n",
    "    result[prefix+'pred'] = pred\n",
    "\n",
    "for name in results.keys():\n",
    "    predict(name)\n",
    "for name in results.keys():\n",
    "    predict(name, isTrain=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(name, isTrain=False):\n",
    "    prefix = 'train' if isTrain else 'test'\n",
    "    result = results[name]\n",
    "    proba = result['model'].predict_proba(result['X_'+prefix])\n",
    "    result[prefix+'pred_prob'] = proba\n",
    "\n",
    "for name in results.keys():\n",
    "    predict_proba(name)\n",
    "for name in results.keys():\n",
    "    predict_proba(name, isTrain=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.figure import Figure\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "def conf_matrix(y_test, y_pred, name, prefix):\n",
    "    cm =  confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=['True', 'False'])\n",
    "    disp.plot()\n",
    "    disp.ax_.set_title(name + ' ' + prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_time(name):\n",
    "    result = results[name]\n",
    "    print(name.upper()+':\\t', result['fit_time'], 'ms')\n",
    "\n",
    "print('Fit execution time')\n",
    "for name in results.keys():\n",
    "    fit_time(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(name, isTrain=False):\n",
    "    prefix = 'train' if isTrain else 'test'\n",
    "    result = results[name]\n",
    "    result[prefix+'score'] = result['model'].score(result['X_'+prefix], result['y_'+prefix])\n",
    "    print(name.upper()+\":\\t\", result[prefix+'score'])\n",
    "\n",
    "print('\\tAccuracy')\n",
    "print(\"Test\")\n",
    "for name in results.keys():\n",
    "    score(name)\n",
    "print(\"\\nTrain\")\n",
    "for name in results.keys():\n",
    "    score(name, isTrain=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision, Recall and F-measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "def evaluate(name, isTrain=False):\n",
    "    prefix = 'train' if isTrain else 'test'\n",
    "    result = results[name]\n",
    "    precision = precision_score(result['y_'+prefix], result[prefix+'pred'], pos_label=-1)\n",
    "    recall = recall_score(result['y_'+prefix], result[prefix+'pred'], pos_label=-1)\n",
    "    f_measure = f1_score(result['y_'+prefix], result[prefix+'pred'], pos_label=-1)\n",
    "    print(prefix, name.upper()+\":\", '\\tRecall:',round(recall, 2), '\\t Precision:',round(precision, 2), '\\tF_Measure:',round(f_measure, 2))\n",
    "\n",
    "print('Test')\n",
    "for name in results.keys():\n",
    "    evaluate(name)\n",
    "print('\\nTrain')\n",
    "for name in results.keys():\n",
    "    evaluate(name, isTrain=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = 0.50\n",
    "\n",
    "prefixes = ('test', 'train')\n",
    "for prefix in prefixes[:1]:\n",
    "    for name in results.keys():\n",
    "        result = results[name]\n",
    "        prefix = 'test'\n",
    "        conf_matrix(result['y_'+prefix], np.where(result[prefix+'pred_prob'][:,-1] > prob, 1, -1), name, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, RocCurveDisplay, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def roc_and_auc(name, isTrain=False):\n",
    "    fig, ax = plt.subplots()\n",
    "    for prefix in ('test', 'train'):\n",
    "        result = results[name]\n",
    "        fpr, tpr, thresholds = roc_curve(result['y_'+prefix], result[prefix+'pred_prob'][:,0], pos_label=result['model'].classes_[0])\n",
    "        roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot(ax=ax,name=(name.upper()+' '+prefix))\n",
    "        print(name.upper(), prefix + \":\\t\", auc(fpr, tpr))\n",
    "\n",
    "for name in results.keys():\n",
    "    roc_and_auc(name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
